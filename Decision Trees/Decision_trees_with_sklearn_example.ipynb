{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f662d169",
   "metadata": {},
   "source": [
    "# Examples: Decision trees with sklearn\n",
    "Â© ExploreAI Academy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26af890c",
   "metadata": {},
   "source": [
    "In this notebook, we will delve into the fundamentals of decision trees and how to implement them using sklearn. We'll cover how decision trees work, the training process, and practical implementation with Python's sklearn library. We will explore the decision-making process, partitioning data, and evaluating model performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d230d14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "* Understand the conceptual structure and functionality of decision trees for regression.\n",
    "* Train a decision tree model using sklearn.\n",
    "* Implement recursive binary splitting to partition data effectively.\n",
    "* Evaluate model performance using mean squared error (MSE).\n",
    "* Visualise decision tree models.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0384b698",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Before we build our first tree, it's important to understand _what_ decision trees are and _how_ they work. Below is a diagram showing the conceptual structure of a decision tree.\n",
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/master/decision_tree_diag.png?raw=true\"  style=\"width:450px\";/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27504ec0",
   "metadata": {},
   "source": [
    "In simple terms, decision trees are tree-like machine learning models which represent data by partitioning it into different sections based on questions asked of predictive variables in the data. \n",
    "\n",
    "To make a decision on a new input:\n",
    "\n",
    "1. We start at the **root node**, which is at the top of an upside-down tree.\n",
    "2. Ask questions at each **decision node** about the attributes of the input.\n",
    "3. Repeat 2. until we reach a **terminal node** (also known as a leaf node) at the bottom of the upside-down tree.\n",
    "\n",
    "\n",
    "Each terminal node in the tree contains a potential output (i.e. $Y$ value) for a given input, $X$. The $Y$ value at a terminal node only becomes the output of our decision tree if the path of decisions on an input $X$ from the root node leads to that particular terminal node."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42dba616",
   "metadata": {},
   "source": [
    "Consider the following example of a new data point $X$ with a value of 4.5. If we wanted to predict the $Y$ value for this new data point, the path that the input would follow along the decision tree is shown in green below:\n",
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/master/decision_tree_path.png?raw=true\"  style=\"width:350px\";/>\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4314e911",
   "metadata": {},
   "source": [
    "After following the path highlighted in green, the data point would eventually be assigned a value of $Y = 2$.   \n",
    "\n",
    "So now we know how to _follow_ a decision tree. But where did the values of the split points come from, and how do we know how many `branches` the tree should have?\n",
    "\n",
    "In the decision process, data points start at the root node and end at a terminal node. The training process follows the same pattern.   \n",
    "\n",
    "In training a decision tree, we begin with all of our training data at the root node and then partition the data into smaller subsets which form the `branches` of the tree.   \n",
    "\n",
    "When partitioning the data, two questions need to be answered:\n",
    "\n",
    "1. Which predictor variable should be used as the split criterion?\n",
    "\n",
    "2. What value of the predictor variable should be used as the splitting point?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed6f8630",
   "metadata": {},
   "source": [
    "### Partitioning\n",
    "\n",
    "Partitioning occurs at every node in a regression tree through a method known as **recursive binary splitting**, employing the following approach:\n",
    "\n",
    "1. **For** each predictor variable **do the following**: \n",
    "\n",
    "    1.1. Evaluate all potential binary data splits, as demonstrated below using Python slicing syntax:\n",
    "    \n",
    "| Left split             | Right split            |\n",
    "|------------------------|------------------------|\n",
    "| x[0]                   | x[1:]                  |\n",
    "| x[:1]                  | x[2:]                  |\n",
    "| x[:2]                  | x[3:]                  |\n",
    "| $\\qquad \\huge \\dots$   | $\\qquad \\huge \\dots$   |\n",
    "| x[:-2]                 | x[-1]                  |\n",
    "    \n",
    "   1.2. Select the data split that maximises data separation, depicted in the figure below.\n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/master/splitting_1.png?raw=true\"  style=\"width:350px\";/>\n",
    "</div>\n",
    "\n",
    "_The optimal split point is determined by its ability to effectively separate the data. For each side of the split (i.e. at a specific value of the predictor variable), the mean $y$ value of the subset is calculated, enabling the computation of the **mean squared error** (MSE). The split point with the minimal MSE on both sides is chosen as the best split._\n",
    "\n",
    "2. Compare the best splits across all predictor variables and select the **optimal** one.\n",
    "\n",
    "3. Divide the data into two subsets based on the selected split.\n",
    "\n",
    "4. Repeat steps 1 to 3 for all subsets until reaching a desired stopping criterion.\n",
    "\n",
    "Following multiple splits, the final partitions may resemble the diagram depicted below:\n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/master/splitting_2.png?raw=true\"  style=\"width:350px\";/>\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0b74630",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d892fee",
   "metadata": {},
   "source": [
    "### Example 1: Visualise the data\n",
    "\n",
    "So now that we have an understanding of how decision trees work, let's implement one using sklearn.\n",
    "\n",
    "Suppose we are tasked with the problem of predicting the price of a house given its area using a dataset called `house_price_by_area`.\n",
    "\n",
    "This dataset contains information on the `LotArea` (in square meters) and the corresponding `SalePrice` (in Rands) of properties.\n",
    "\n",
    "We'll start by importing some commonly used Python libraries and loading our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6477dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # used for mathematical operations\n",
    "import pandas as pd  # for loading CSV data\n",
    "import matplotlib.pyplot as plt  # for plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a760555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://github.com/Explore-AI/Public-Data/blob/master/house_price_by_area.csv?raw=true\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8be0f03",
   "metadata": {},
   "source": [
    "We will be using the value of `LotArea` (independent variable) to try and predict the `SalePrice` (dependent variable).   \n",
    "\n",
    "Let's take a look at the data using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"LotArea\"] # independent variable\n",
    "y = df[\"SalePrice\"] # dependent variable\n",
    "\n",
    "plt.scatter(X,y) # create scatter plot\n",
    "plt.title(\"House Price vs Area\")\n",
    "plt.xlabel(\"Lot Area in m$^2$\")\n",
    "plt.ylabel(\"Sale Price in Rands\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca947437",
   "metadata": {},
   "source": [
    "### Example 2: Train-test split\n",
    "Next we split our dataset into training and testing sets so that we can later evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f0fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e24fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X to a numpy array and then reshape it to add a new axis\n",
    "# set test size to 20 % of training data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X.values.reshape(-1, 1), y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83509796",
   "metadata": {},
   "source": [
    "### Example 3: Building the decision tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fe111be",
   "metadata": {},
   "source": [
    "Let's import the `DecisionTreeRegressor` class from the tree module of the sklearn library which will help us fit a decision tree to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4689cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7032ce1e",
   "metadata": {},
   "source": [
    "After importing our Decision Tree Regressor model, we have to instantiate the model. This step also allows us to set model hyperparameters such as:\n",
    "\n",
    "- **max_depth**: The maximum depth of the tree (i.e. the number of nodes between root and leaf node).\n",
    "- **criterion**: The function to measure the quality of a split. The model uses the mean squared error (mse) by default.\n",
    "- **random_state**: A number used to seed the random number generator. Ensures that we get the same tree each time we call model.fit(); among other hyperparameters.\n",
    "\n",
    "To learn more about other DecisionTreeRegressor hyperparameters and descriptions, run `help(DecisionTreeRegressor)` in a new cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d6757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate regression tree model\n",
    "regr_tree = DecisionTreeRegressor(max_depth=2,random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3babee3",
   "metadata": {},
   "source": [
    "As with most sklearn models, we call `.fit()` to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_tree.fit(x_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5d11597",
   "metadata": {},
   "source": [
    "`sklearn` provides a useful feature for visualising a fitted decision tree model. We can access it by importing the `plot_tree` method from the `sklearn.tree` submodule. Take note of the split points for the `LotArea` variable, as well as the MSE values and the number of samples in each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "# assigned a random variable name to the plot to suppress text output\n",
    "_ = plot_tree(regr_tree, feature_names=['LotArea'],  filled=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d35aef7",
   "metadata": {},
   "source": [
    "At this point, we have a fully trained decision tree model and can easily make predictions by calling the `.predict` method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30a15e4c",
   "metadata": {},
   "source": [
    "### Example 4: Evaluating model performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f1c6e50",
   "metadata": {},
   "source": [
    "We evaluate model performance by comparing the model's predictions on unseen data `(x_test)` with the actual output `(y_test)`. This comparison helps assess how well the model generalises to new data and provides insights into its predictive accuracy. We will use `mean_squared_error` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc3d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "# get predictions for test data\n",
    "y_pred = regr_tree.predict(x_test)\n",
    "\n",
    "# calculate MSE\n",
    "MSE = mean_squared_error(y_pred,y_test)\n",
    "\n",
    "# Report RMSE\n",
    "print(\"Regression decision tree model RMSE is:\",np.sqrt(MSE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95edd111",
   "metadata": {},
   "source": [
    "### Example 5: Visualising model output\n",
    "\n",
    "\n",
    "One way to visualise a regression decision tree model is to obtain predictions $\\hat{y}$ for a range of $x$ values and then plot the resulting step function. In this case, we can define our domain as a set of equidistant points along the $x$-axis, spanning from the minimum to the maximum $x$-values in the dataset. This approach captures the segmented nature of the decision tree predictions, where each segment corresponds to a different region of the predictor variable space determined by the splits in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec509b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate a range of equidistant points along the x-axis spanning from the minimum\n",
    "# to the maximum X-values in the dataset, consisting of 100 points.\n",
    "x_domain = np.linspace(min(X), max(X), 100)[:, np.newaxis] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec05aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict y for every point in x-domain\n",
    "y_predictions = regr_tree.predict(x_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862f4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the regression tree line over data\n",
    "plt.figure()\n",
    "plt.scatter(X, y)\n",
    "plt.plot(x_domain, y_predictions, color=\"red\", label='predictions')\n",
    "plt.xlabel(\"LotArea in m$^2$\")\n",
    "plt.ylabel(\"SalePrice in Rands\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a262dfd6",
   "metadata": {},
   "source": [
    "In a step function plot, the model's predictions are represented by discrete steps rather than smooth curves. Each step corresponds to a particular interval or region defined by the predictor variables in the dataset.\n",
    "\n",
    "The abrupt transitions between steps indicate where the model changes its prediction based on the splitting criteria learned during training. These transitions represent the decision boundaries of the tree, where the predictor variables are partitioned into subsets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09b12c2e",
   "metadata": {},
   "source": [
    "### Advantages and disadvantages of decision trees\n",
    "\n",
    "Advantages:\n",
    "\n",
    "* Easy to understand and use.\n",
    "* Capable of handling both categorical and numerical data.\n",
    "* Resilient to outliers, requiring minimal data preprocessing.\n",
    "* Flexible for incorporating new features and can be extended to build larger classifiers using ensemble methods.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "* Prone to overfitting, especially with complex datasets.\n",
    "* Requires careful parameter tuning to optimise performance.\n",
    "* Susceptible to bias if certain classes dominate the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd094b-0fee-46f1-a4b8-73766813c42b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
