{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xpIPMNPoU_-p"
   },
   "source": [
    "# Examples: An introduction to NLP and cleaning text\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore text-cleaning and feature-extraction techniques in NLP. We'll use the NLTK library to preprocess unstructured text data, preparing it for machine learning tasks. By using examples, we'll work through the steps of cleaning text data and extracting meaningful features.\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "\n",
    "* Gain a basic understanding of text-cleaning techniques.\n",
    "* Implement text-cleaning steps such as removing URLs, converting text to lowercase, and removing punctuation.\n",
    "* Understand the concept of tokenisation and its importance in text processing.\n",
    "* Apply stemming and lemmatization to reduce words to their root forms.\n",
    "* Demonstrate the removal of stop words and their impact on text analysis.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial introduces basic concepts in natural language processing, and in particular, common techniques for handling, processing, and preparing unstructured text data for use with machine learning models. The concepts introduced here are also useful for text analysis, so please feel free to do more research and see what can be achieved using the MBTI dataset.\n",
    "\n",
    "Before diving in, let's acquire the necessary data and the primary library we'll be utilising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lKmhMwlDU_-u"
   },
   "source": [
    "## NLTK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubXxX-njU_-u"
   },
   "source": [
    "NLTK – Natural Language Toolkit – is a widely used library for building Python programs to work with human language data. It provides interfaces to numerous corpora and lexical resources, such as WordNet, along with a suite of text-processing libraries for classification, tokenisation, stemming, tagging, parsing, and semantic reasoning. Additionally, NLTK offers wrappers for various NLP libraries and features an active discussion forum.\n",
    "\n",
    "Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation, NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike. NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open-source, community-driven project.\n",
    "\n",
    "Let's import `nltk` and other packages to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQm0O5XHU_-z",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# set plot style\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading NLTK corpora"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C68jzOFpU_-2"
   },
   "source": [
    "\n",
    "Several text processing methods introduced in the NLTK require accessing predefined language resources, such as [stop word lists](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/). For instance, when identifying stop words within a text, NLTK relies on a [corpus](https://en.wikipedia.org/wiki/Text_corpus) containing such words. This corpus serves as a reference for the lookup operation during text processing. To ensure seamless execution of NLTK methods, it's essential to download the required corpora beforehand. Failure to do so may result in lookup errors during tokenisation and stop word removal processes. Fortunately, we can pre-emptively address these issues by downloading the necessary corpora using the NLTK downloader tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29356,
     "status": "error",
     "timestamp": 1560340175121,
     "user": {
      "displayName": "Bryan Davies",
      "photoUrl": "",
      "userId": "03059035420523728518"
     },
     "user_tz": -120
    },
    "id": "w8Iw1yCRU_-2",
    "outputId": "188501d1-fcf6-45be-8a45-56f885491dcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK corpora\n",
    "nltk.download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lct3bK9aU_-7"
   },
   "source": [
    "You should see a pop-up box similar to the one shown below.\n",
    "\n",
    "Note: The box might appear in the background, in which case you can use alt + tab to switch to the downloader window.\n",
    "\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/master/nltk_downloader.png?raw=true\" width=50%/> \n",
    "\n",
    "Use it to navigate to the items we need to download:\n",
    "\n",
    "* Stopwords corpus (under the Corpora tab)\n",
    "* Punkt tokenizer models (under the Models tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HhoRTAwgU_-7",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or we can download directly, i.e.\n",
    "nltk.download(['punkt','stopwords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the download was successful, then the following import should work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the Process of CLeaning Text (Text Pre-Processing)\n",
    "\n",
    "# 1. Case Normalization - Converting the entire text string to a single case (Recommended to Lowercase the Text string)\n",
    "# 2. Punctuation and Special Character removal. Since they may be trated as tokens, reducing the size of vocuabulary. \n",
    "# 3. Tokenization - Breating strings of text into smaller chunks called tokens.\n",
    "# 4. Stop Words Removal -  Removing the words that have very litle sentimental meaning. Essential for reducing dimensions \n",
    "# 5.Stemming & Lemmaatization - Reducing the inflected words in a text to thier core form so that similar words are treated a single token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we'll explore stop words in greater detail later in this train, it won't hurt to take a quick look at what we've downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of English stop words\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MBTI dataset\n",
    "\n",
    "The Myers Briggs Type Indicator (or MBTI for short) is a personality type system that divides people into one of 16 distinct personality types across 4 axes:\n",
    "\n",
    "    Introversion (I) – Extroversion (E)\n",
    "    Intuition (N) – Sensing (S)\n",
    "    Thinking (T) – Feeling (F)\n",
    "    Judging (J) – Perceiving (P)\n",
    "\n",
    "[(More can be learned about what these mean here.)](https://www.myersbriggs.org/my-mbti-personality-type/mbti-basics/home.htm)\n",
    "\n",
    "So, for example, someone who prefers introversion, intuition, thinking, and perceiving would be labelled an INTP in the MBTI system, and there are lots of personality-based components that would model or describe this person’s preferences or behaviour based on their label.\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/1/1f/MyersBriggsTypes.png'>\n",
    "\n",
    "Image by Jake Beech, [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0)\n",
    "\n",
    "In this train, we'll use a version of [the MBTI dataset](https://www.kaggle.com/datasnaek/mbti-type) which contains over 6000 rows of data, where on each row is a person’s:\n",
    "\n",
    " - MBTI type (four-letter MBTI code).\n",
    " - A section of each of the last 50 things they have posted online (each entry separated by \"|||\" (three pipe characters)).   \n",
    " \n",
    "_**Note:** If you are curious, you can find out what your MBTI personality is by taking the test here: https://www.16personalities.com/_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKeKPQWkU_-9"
   },
   "source": [
    "### Let's get the data and clean it up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 763,
     "status": "error",
     "timestamp": 1560340182462,
     "user": {
      "displayName": "Bryan Davies",
      "photoUrl": "",
      "userId": "03059035420523728518"
     },
     "user_tz": -120
    },
    "id": "1g8hjHxFU_-9",
    "outputId": "07b8ba22-a4e3-4d73-c121-55cd4a827b10"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the MBTI dataset\n",
    "mbti = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint/mbti_train.csv')\n",
    "mbti.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by printing a list of all the MBTI personality types that are present in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP', 'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n"
     ]
    }
   ],
   "source": [
    "# Print list of unique MBTI personality types\n",
    "type_labels = list(mbti.type.unique())\n",
    "print(type_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDSv8UyDU__G"
   },
   "source": [
    "Let's have a look at how many data samples we have for each of the different MBTI personality types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INFJ',\n",
       " 'ENTP',\n",
       " 'INTP',\n",
       " 'INTJ',\n",
       " 'ENTJ',\n",
       " 'ENFJ',\n",
       " 'INFP',\n",
       " 'ENFP',\n",
       " 'ISFP',\n",
       " 'ISTP',\n",
       " 'ISFJ',\n",
       " 'ISTJ',\n",
       " 'ESTP',\n",
       " 'ESFP',\n",
       " 'ESTJ',\n",
       " 'ESFJ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type for type in mbti.type.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "INFP    21.303412\n",
       "INFJ    16.907470\n",
       "INTP    14.755610\n",
       "INTJ    12.757455\n",
       "ENTP     8.146326\n",
       "ENFP     7.623732\n",
       "ISTP     3.919459\n",
       "ISFP     3.043345\n",
       "ENTJ     2.566861\n",
       "ISTJ     2.228712\n",
       "ENFJ     2.197971\n",
       "ISFJ     1.905933\n",
       "ESTP     1.091300\n",
       "ESFP     0.553335\n",
       "ESFJ     0.537965\n",
       "ESTJ     0.461113\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti.type.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h3G_aB4hU__G",
    "outputId": "92110f2c-5cab-4f7b-a5b3-4a9b0be103e3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise the distribution of MBTI personality types\n",
    "mbti['type'].value_counts().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z7h-hcmsU__J"
   },
   "source": [
    "It looks like we have very few samples for the 'ES' types. Maybe because they are out in the real world, not sitting behind a computer screen! :)   \n",
    "   \n",
    "Let's increase the size of the dataset by separating each of the 50 posts in the `posts` column of each row into its own row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88DRAzHnU__J",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate each post in the 'posts' column into its own row\n",
    "all_mbti = []\n",
    "for i, row in mbti.iterrows():\n",
    "    for post in row['posts'].split('|||'):\n",
    "        all_mbti.append([row['type'], post])\n",
    "all_mbti = pd.DataFrame(all_mbti, columns=['type', 'post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hF5aiupbU__M",
    "outputId": "897367e2-ffbb-4ced-f418-54ed1178e646"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316548, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many rows do we have now?\n",
    "all_mbti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://41.media.tumblr.com/tumblr_lfouy03PMA1q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp and intj moments  https://www.youtube.com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>What has been the most life-changing experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://www.youtube.com/watch?v=vXZeYwwRDw8   h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               post\n",
       "0  INFJ        'http://www.youtube.com/watch?v=qsXHcwe3krw\n",
       "1  INFJ  http://41.media.tumblr.com/tumblr_lfouy03PMA1q...\n",
       "2  INFJ  enfp and intj moments  https://www.youtube.com...\n",
       "3  INFJ  What has been the most life-changing experienc...\n",
       "4  INFJ  http://www.youtube.com/watch?v=vXZeYwwRDw8   h..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mbti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bElSlHKGU__W",
    "outputId": "182b0f14-6348-49f7-ff80-63515865a30a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_mbti['type'].value_counts().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnKe3gyoU__Z"
   },
   "source": [
    "Although the proportions of the classes remain consistent, we have significantly increased the number of samples for the 'ES' personality types by separating each post into its own row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCXae5QXU__Z"
   },
   "source": [
    "## Text cleaning\n",
    "\n",
    "### Removing noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwJMs441U__a"
   },
   "source": [
    "In text analytics, removing noise (i.e. unnecessary information) is a key part of getting the data into a usable format.  Some techniques are standard, but your own unique dataset will require some creative thinking on your part.\n",
    "\n",
    "For the MBTI dataset, we will be doing the following:\n",
    "* Remove the web-urls.\n",
    "* Make all the text lowercase.\n",
    "* Remove punctuation.\n",
    "\n",
    "**[Regular expressions](https://www.regular-expressions.info/)** can be very useful for extracting information from text.  If you feel brave, go teach yourself all about it. If not, just follow along.  This next step effectively removes all websites and replaces them with the text `'web-url'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lwQ40KKDU__a",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Replace URLs in the 'post' column with a placeholder string\n",
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r'url-web'\n",
    "all_mbti['post'] = all_mbti['post'].replace(to_replace = pattern_url, value = subs_url, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mbti = mbti.copy(deep=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pattern = r\"http[s]?://[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f[0-9A-Fa-f]])+\" \n",
    "\n",
    "my_mbti[\"post\"] = my_mbti.posts.replace(regex=True, to_replace=url_pattern, value=\"web-url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>web-urlweb-urlwwweb-urlyoutubeweb-urlcomweb-ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>web-urlweb-urlweb-urlm finding the lack of me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>web-urlweb-urlood one  web-urlweb-urlweb-urlwe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>web-urlweb-urlear web-urlweb-urlweb-urlweb-url...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>web-urlweb-urlouweb-urlre firedweb-url|||web-u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  \\\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...   \n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                                post  \n",
       "0  web-urlweb-urlwwweb-urlyoutubeweb-urlcomweb-ur...  \n",
       "1  web-urlweb-urlweb-urlm finding the lack of me ...  \n",
       "2  web-urlweb-urlood one  web-urlweb-urlweb-urlwe...  \n",
       "3  web-urlweb-urlear web-urlweb-urlweb-urlweb-url...  \n",
       "4  web-urlweb-urlouweb-urlre firedweb-url|||web-u...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mbti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLOKzwsdU__d",
    "outputId": "04f9009e-4ec0-4e6d-91a5-9f5fb59b497f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'url-web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>url-web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp and intj moments  url-web  sportscenter n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>What has been the most life-changing experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>url-web   url-web  On repeat for most of today.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               post\n",
       "0  INFJ                                           'url-web\n",
       "1  INFJ                                            url-web\n",
       "2  INFJ  enfp and intj moments  url-web  sportscenter n...\n",
       "3  INFJ  What has been the most life-changing experienc...\n",
       "4  INFJ    url-web   url-web  On repeat for most of today."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mbti.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-anTzPrnU__h"
   },
   "source": [
    "**Food for thought...** There seem to be a lot of YouTube and other links embedded.  Maybe you can think of ways to collect even more information from these links. How about page titles and names of YouTube videos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wzM8TbWBU__h"
   },
   "source": [
    "### Remove punctuation\n",
    "\n",
    "First we make all the text lowercase to remove some noise from capitalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CUATZmo5U__h",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_mbti['post'] = all_mbti['post'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove the punctuation using the `string` import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gvhXxcHzU__j",
    "outputId": "7d82c9f5-e448-4f1c-edf9-c75493ea7cf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-RPGgE5U__l",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(post):\n",
    "    return ''.join([l for l in post if l not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2sFFdyNCU__n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'just when i think i’ve lost you just when i’m so tired i toss away the fight and say “i’ll just embrace my demons then… ‘cause you feel so far away and i’ll never be your angel” —that’s when'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mbti['post'] = all_mbti['post'].apply(remove_punctuation)\n",
    "all_mbti['post'].iloc[268558]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like some punctuation snuck in! See if you can figure out why. Hint: It has something to do with the standard encoding on text files in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FvA-QZmRU__r"
   },
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sRDkqnGgU__s"
   },
   "source": [
    "A tokeniser divides text into a sequence of tokens, which roughly correspond to \"words\" (see the [Stanford Tokeniser](https://nlp.stanford.edu/software/tokenizer.html)). We will use tokenisers to clean up the data, making it ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X9nnIjyhU__t",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize, TreebankWordTokenizer \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpopular\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning Notebooks\\venv\\Lib\\site-packages\\nltk\\downloader.py:762\u001b[39m, in \u001b[36mDownloader.download\u001b[39m\u001b[34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[39m\n\u001b[32m    753\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(s, prefix2=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    754\u001b[39m     print_to(\n\u001b[32m    755\u001b[39m         textwrap.fill(\n\u001b[32m    756\u001b[39m             s,\n\u001b[32m   (...)\u001b[39m\u001b[32m    759\u001b[39m         )\n\u001b[32m    760\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m762\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mincr_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo_or_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Error messages\u001b[39;49;00m\n\u001b[32m    764\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mErrorMessage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning Notebooks\\venv\\Lib\\site-packages\\nltk\\downloader.py:625\u001b[39m, in \u001b[36mDownloader.incr_download\u001b[39m\u001b[34m(self, info_or_id, download_dir, force)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(info, Collection):\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m StartCollectionMessage(info)\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.incr_download(info.children, download_dir, force)\n\u001b[32m    626\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m FinishCollectionMessage(info)\n\u001b[32m    628\u001b[39m \u001b[38;5;66;03m# Handle Packages (delegate to a helper function).\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning Notebooks\\venv\\Lib\\site-packages\\nltk\\downloader.py:612\u001b[39m, in \u001b[36mDownloader.incr_download\u001b[39m\u001b[34m(self, info_or_id, download_dir, force)\u001b[39m\n\u001b[32m    610\u001b[39m \u001b[38;5;66;03m# If they gave us a list of ids, then download each one.\u001b[39;00m\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(info_or_id, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._download_list(info_or_id, download_dir, force)\n\u001b[32m    613\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    615\u001b[39m \u001b[38;5;66;03m# Look up the requested collection or package.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning Notebooks\\venv\\Lib\\site-packages\\nltk\\downloader.py:655\u001b[39m, in \u001b[36mDownloader._download_list\u001b[39m\u001b[34m(self, items, download_dir, force)\u001b[39m\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    654\u001b[39m     delta = \u001b[38;5;28mlen\u001b[39m(item.packages) / num_packages\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mincr_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mProgressMessage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mProgressMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning Notebooks\\venv\\Lib\\site-packages\\nltk\\downloader.py:630\u001b[39m, in \u001b[36mDownloader.incr_download\u001b[39m\u001b[34m(self, info_or_id, download_dir, force)\u001b[39m\n\u001b[32m    626\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m FinishCollectionMessage(info)\n\u001b[32m    628\u001b[39m \u001b[38;5;66;03m# Handle Packages (delegate to a helper function).\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._download_package(info, download_dir, force)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning Notebooks\\venv\\Lib\\site-packages\\nltk\\downloader.py:698\u001b[39m, in \u001b[36mDownloader._download_package\u001b[39m\u001b[34m(self, info, download_dir, force)\u001b[39m\n\u001b[32m    696\u001b[39m num_blocks = \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, info.size / (\u001b[32m1024\u001b[39m * \u001b[32m16\u001b[39m))\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m itertools.count():\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m     s = \u001b[43minfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 16k blocks.\u001b[39;00m\n\u001b[32m    699\u001b[39m     outfile.write(s)\n\u001b[32m    700\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:465\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    463\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    464\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m s = \u001b[38;5;28mself\u001b[39m.fp.read(amt)\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    467\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    468\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    469\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    708\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1278\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1275\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1276\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1277\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1134\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1134\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer \n",
    "\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "stR6cyC_U__v",
    "outputId": "2c2ed231-656c-420f-dd04-fe19d8df1d09"
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\lenovo/nltk_data'\n    - 'd:\\\\Machine Learning Notebooks\\\\venv\\\\nltk_data'\n    - 'd:\\\\Machine Learning Notebooks\\\\venv\\\\share\\\\nltk_data'\n    - 'd:\\\\Machine Learning Notebooks\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lenovo\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mA tokenizer divides text into a sequence of tokens, which roughly correspond to \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwords\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning Notebooks\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning Notebooks\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning Notebooks\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning Notebooks\\venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning Notebooks\\venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning Notebooks\\venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\lenovo/nltk_data'\n    - 'd:\\\\Machine Learning Notebooks\\\\venv\\\\nltk_data'\n    - 'd:\\\\Machine Learning Notebooks\\\\venv\\\\share\\\\nltk_data'\n    - 'd:\\\\Machine Learning Notebooks\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lenovo\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "word_tokenize('A tokenizer divides text into a sequence of tokens, which roughly correspond to \"words\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the TreeBankWordTokenizer since it is MUCH quicker than the word_tokenize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2rChKdjU__y",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Tokenise the text using the TreebankWordTokenizer\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "all_mbti['tokens'] = all_mbti['post'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>post</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>urlweb</td>\n",
       "      <td>[urlweb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>urlweb</td>\n",
       "      <td>[urlweb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp and intj moments  urlweb  sportscenter no...</td>\n",
       "      <td>[enfp, and, intj, moments, urlweb, sportscente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>what has been the most lifechanging experience...</td>\n",
       "      <td>[what, has, been, the, most, lifechanging, exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>urlweb   urlweb  on repeat for most of today</td>\n",
       "      <td>[urlweb, urlweb, on, repeat, for, most, of, to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               post  \\\n",
       "0  INFJ                                             urlweb   \n",
       "1  INFJ                                             urlweb   \n",
       "2  INFJ  enfp and intj moments  urlweb  sportscenter no...   \n",
       "3  INFJ  what has been the most lifechanging experience...   \n",
       "4  INFJ       urlweb   urlweb  on repeat for most of today   \n",
       "\n",
       "                                              tokens  \n",
       "0                                           [urlweb]  \n",
       "1                                           [urlweb]  \n",
       "2  [enfp, and, intj, moments, urlweb, sportscente...  \n",
       "3  [what, has, been, the, most, lifechanging, exp...  \n",
       "4  [urlweb, urlweb, on, repeat, for, most, of, to...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = all_mbti.copy(deep=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization - Break large text into smaller units called tokens \n",
    "my_tokenizer = TreebankWordTokenizer()\n",
    "dataset[\"tokens\"] = dataset.post.apply(my_tokenizer.tokenize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['enfp',\n",
       " 'and',\n",
       " 'intj',\n",
       " 'moments',\n",
       " 'urlweb',\n",
       " 'sportscenter',\n",
       " 'not',\n",
       " 'top',\n",
       " 'ten',\n",
       " 'plays',\n",
       " 'urlweb',\n",
       " 'pranks']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tokens.iloc[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8o8u2kwU__1",
    "outputId": "cf9aad76-1846-4af1-aef1-399bd4a31edc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'find',\n",
       " 'all',\n",
       " 'of',\n",
       " 'you',\n",
       " 'to',\n",
       " 'be',\n",
       " 'extremely',\n",
       " 'humorous',\n",
       " 'now',\n",
       " 'to',\n",
       " 'find',\n",
       " 'other',\n",
       " 'specimen',\n",
       " 'to',\n",
       " 'observe']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mbti['tokens'].iloc[55555]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hAUkklVXU__6"
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qlw3iqz3U__7"
   },
   "source": [
    "Stemming is the process of transforming to the root word. It uses an algorithm that removes\n",
    "common word endings from English words, such as “ly”, “es”, “ed”, and “s”. \n",
    "\n",
    "For instance, suppose you're conducting an analysis and you wish to treat words like \"carefully\", \"cared\", \"cares\", and \"caringly\" as a single entity, \"care\", rather than separate words. There are three widely used stemming algorithms, namely:\n",
    "* Porter\n",
    "* Lancaster\n",
    "* Snowball\n",
    "\n",
    "Out of these three, we will be using the `SnowballStemmer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BRY-EGeuU__7",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5hmBs_DU__-",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "words = 'caring cares cared caringly carefully'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "care\n",
      "care\n",
      "care\n",
      "care\n",
      "care\n",
      "careful\n"
     ]
    }
   ],
   "source": [
    "my_words = \"care cares caringly cared careful carefull\" \n",
    "my_stemmer = SnowballStemmer(\"english\")\n",
    "for word in my_words.split():\n",
    "    print(my_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILU4MAgaVAAA",
    "outputId": "e012205b-1a2a-441a-f614-1dd1ee498ce6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "care\n",
      "care\n",
      "care\n",
      "care\n",
      "care\n"
     ]
    }
   ],
   "source": [
    "# find the stem of each word in words\n",
    "stemmer = SnowballStemmer('english')\n",
    "for word in words.split():\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMplementing Stemming \n",
    "def stemmer(words_list, stemmer):\n",
    "    return [stemmer.stem(word) for word in words_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"stem\"] = dataset.tokens.apply(stemmer, args=(my_stemmer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>post</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>urlweb</td>\n",
       "      <td>[urlweb]</td>\n",
       "      <td>[urlweb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>urlweb</td>\n",
       "      <td>[urlweb]</td>\n",
       "      <td>[urlweb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp and intj moments  urlweb  sportscenter no...</td>\n",
       "      <td>[enfp, and, intj, moments, urlweb, sportscente...</td>\n",
       "      <td>[enfp, and, intj, moment, urlweb, sportscent, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>what has been the most lifechanging experience...</td>\n",
       "      <td>[what, has, been, the, most, lifechanging, exp...</td>\n",
       "      <td>[what, has, been, the, most, lifechang, experi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>urlweb   urlweb  on repeat for most of today</td>\n",
       "      <td>[urlweb, urlweb, on, repeat, for, most, of, to...</td>\n",
       "      <td>[urlweb, urlweb, on, repeat, for, most, of, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>may the perc experience immerse you</td>\n",
       "      <td>[may, the, perc, experience, immerse, you]</td>\n",
       "      <td>[may, the, perc, experi, immers, you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>the last thing my infj friend posted on his fa...</td>\n",
       "      <td>[the, last, thing, my, infj, friend, posted, o...</td>\n",
       "      <td>[the, last, thing, my, infj, friend, post, on,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>hello enfj7 sorry to hear of your distress its...</td>\n",
       "      <td>[hello, enfj7, sorry, to, hear, of, your, dist...</td>\n",
       "      <td>[hello, enfj7, sorri, to, hear, of, your, dist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>84389  84390  urlweb  urlweb</td>\n",
       "      <td>[84389, 84390, urlweb, urlweb]</td>\n",
       "      <td>[84389, 84390, urlweb, urlweb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>welcome and stuff</td>\n",
       "      <td>[welcome, and, stuff]</td>\n",
       "      <td>[welcom, and, stuff]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               post  \\\n",
       "0  INFJ                                             urlweb   \n",
       "1  INFJ                                             urlweb   \n",
       "2  INFJ  enfp and intj moments  urlweb  sportscenter no...   \n",
       "3  INFJ  what has been the most lifechanging experience...   \n",
       "4  INFJ       urlweb   urlweb  on repeat for most of today   \n",
       "5  INFJ                may the perc experience immerse you   \n",
       "6  INFJ  the last thing my infj friend posted on his fa...   \n",
       "7  INFJ  hello enfj7 sorry to hear of your distress its...   \n",
       "8  INFJ                      84389  84390  urlweb  urlweb    \n",
       "9  INFJ                                  welcome and stuff   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                                           [urlweb]   \n",
       "1                                           [urlweb]   \n",
       "2  [enfp, and, intj, moments, urlweb, sportscente...   \n",
       "3  [what, has, been, the, most, lifechanging, exp...   \n",
       "4  [urlweb, urlweb, on, repeat, for, most, of, to...   \n",
       "5         [may, the, perc, experience, immerse, you]   \n",
       "6  [the, last, thing, my, infj, friend, posted, o...   \n",
       "7  [hello, enfj7, sorry, to, hear, of, your, dist...   \n",
       "8                     [84389, 84390, urlweb, urlweb]   \n",
       "9                              [welcome, and, stuff]   \n",
       "\n",
       "                                                stem  \n",
       "0                                           [urlweb]  \n",
       "1                                           [urlweb]  \n",
       "2  [enfp, and, intj, moment, urlweb, sportscent, ...  \n",
       "3  [what, has, been, the, most, lifechang, experi...  \n",
       "4  [urlweb, urlweb, on, repeat, for, most, of, to...  \n",
       "5              [may, the, perc, experi, immers, you]  \n",
       "6  [the, last, thing, my, infj, friend, post, on,...  \n",
       "7  [hello, enfj7, sorri, to, hear, of, your, dist...  \n",
       "8                     [84389, 84390, urlweb, urlweb]  \n",
       "9                               [welcom, and, stuff]  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us stem all of the words in the MBTI DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0iw9106FVAAE",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def mbti_stemmer(words, stemmer):\n",
    "    return [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NjTuPQNOVAAF"
   },
   "outputs": [],
   "source": [
    "all_mbti['stem'] = all_mbti['tokens'].apply(mbti_stemmer, args=(stemmer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>post</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>urlweb</td>\n",
       "      <td>[urlweb]</td>\n",
       "      <td>[urlweb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>urlweb</td>\n",
       "      <td>[urlweb]</td>\n",
       "      <td>[urlweb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp and intj moments  urlweb  sportscenter no...</td>\n",
       "      <td>[enfp, and, intj, moments, urlweb, sportscente...</td>\n",
       "      <td>[enfp, and, intj, moment, urlweb, sportscent, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>what has been the most lifechanging experience...</td>\n",
       "      <td>[what, has, been, the, most, lifechanging, exp...</td>\n",
       "      <td>[what, has, been, the, most, lifechang, experi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>urlweb   urlweb  on repeat for most of today</td>\n",
       "      <td>[urlweb, urlweb, on, repeat, for, most, of, to...</td>\n",
       "      <td>[urlweb, urlweb, on, repeat, for, most, of, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>may the perc experience immerse you</td>\n",
       "      <td>[may, the, perc, experience, immerse, you]</td>\n",
       "      <td>[may, the, perc, experi, immers, you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>the last thing my infj friend posted on his fa...</td>\n",
       "      <td>[the, last, thing, my, infj, friend, posted, o...</td>\n",
       "      <td>[the, last, thing, my, infj, friend, post, on,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>hello enfj7 sorry to hear of your distress its...</td>\n",
       "      <td>[hello, enfj7, sorry, to, hear, of, your, dist...</td>\n",
       "      <td>[hello, enfj7, sorri, to, hear, of, your, dist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>84389  84390  urlweb  urlweb</td>\n",
       "      <td>[84389, 84390, urlweb, urlweb]</td>\n",
       "      <td>[84389, 84390, urlweb, urlweb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>welcome and stuff</td>\n",
       "      <td>[welcome, and, stuff]</td>\n",
       "      <td>[welcom, and, stuff]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               post  \\\n",
       "0  INFJ                                             urlweb   \n",
       "1  INFJ                                             urlweb   \n",
       "2  INFJ  enfp and intj moments  urlweb  sportscenter no...   \n",
       "3  INFJ  what has been the most lifechanging experience...   \n",
       "4  INFJ       urlweb   urlweb  on repeat for most of today   \n",
       "5  INFJ                may the perc experience immerse you   \n",
       "6  INFJ  the last thing my infj friend posted on his fa...   \n",
       "7  INFJ  hello enfj7 sorry to hear of your distress its...   \n",
       "8  INFJ                      84389  84390  urlweb  urlweb    \n",
       "9  INFJ                                  welcome and stuff   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                                           [urlweb]   \n",
       "1                                           [urlweb]   \n",
       "2  [enfp, and, intj, moments, urlweb, sportscente...   \n",
       "3  [what, has, been, the, most, lifechanging, exp...   \n",
       "4  [urlweb, urlweb, on, repeat, for, most, of, to...   \n",
       "5         [may, the, perc, experience, immerse, you]   \n",
       "6  [the, last, thing, my, infj, friend, posted, o...   \n",
       "7  [hello, enfj7, sorry, to, hear, of, your, dist...   \n",
       "8                     [84389, 84390, urlweb, urlweb]   \n",
       "9                              [welcome, and, stuff]   \n",
       "\n",
       "                                                stem  \n",
       "0                                           [urlweb]  \n",
       "1                                           [urlweb]  \n",
       "2  [enfp, and, intj, moment, urlweb, sportscent, ...  \n",
       "3  [what, has, been, the, most, lifechang, experi...  \n",
       "4  [urlweb, urlweb, on, repeat, for, most, of, to...  \n",
       "5              [may, the, perc, experi, immers, you]  \n",
       "6  [the, last, thing, my, infj, friend, post, on,...  \n",
       "7  [hello, enfj7, sorri, to, hear, of, your, dist...  \n",
       "8                     [84389, 84390, urlweb, urlweb]  \n",
       "9                               [welcom, and, stuff]  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mbti.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the results of the stemmer to see what we have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i                    --> i         \n",
      "hate                 --> hate      \n",
      "april                --> april     \n",
      "fools                --> fool      \n",
      "day                  --> day       \n",
      "angry                --> angri     \n",
      "theres               --> there     \n",
      "a                    --> a         \n",
      "site                 --> site      \n",
      "im                   --> im        \n",
      "regularly            --> regular   \n",
      "on                   --> on        \n",
      "and                  --> and       \n",
      "the                  --> the       \n",
      "admins               --> admin     \n",
      "are                  --> are       \n",
      "screwing             --> screw     \n",
      "everything           --> everyth   \n",
      "up                   --> up        \n",
      "today                --> today     \n",
      "for                  --> for       \n",
      "a                    --> a         \n",
      "laugh                --> laugh     \n",
      "but                  --> but       \n",
      "i                    --> i         \n",
      "dont                 --> dont      \n",
      "find                 --> find      \n",
      "it                   --> it        \n",
      "funny                --> funni     \n",
      "im                   --> im        \n",
      "actually             --> actual    \n",
      "quite                --> quit      \n",
      "freaked              --> freak     \n",
      "out                  --> out       \n",
      "about                --> about     \n",
      "it                   --> it        \n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(all_mbti.iloc[268702]['tokens']):    \n",
    "    print ('{:20s} --> {:10s}'.format(t, all_mbti.iloc[268702]['stem'][i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFln-NFtVAAI"
   },
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization is another text normalisation technique that aims to reduce words to their base or dictionary form, known as lemmas. Unlike stemming, which simply removes word endings, lemmatization considers the context of the word and morphological analysis to accurately derive the lemma. This ensures that the resulting lemma is a valid word found in the dictionary, preserving the semantic meaning of the word in different contexts. While lemmatization is more computationally intensive compared to stemming, it offers higher accuracy in maintaining the integrity of words.\n",
    "\n",
    "Sometimes, we may end up with a word that closely resembles the original, while in other cases, we might get a word that is entirely different. Let's explore some examples to illustrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nivlmH3rVAAJ",
    "outputId": "44c3133a-81b6-4c75-cc46-f4e8ef7f387c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"ran\",'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's lemmatize all of the words in the MBTI DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the Lematization Approach \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_lematization(word_list, func):\n",
    "    return [my_lemmatizer.lemmatize(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"lemma\"] = dataset.tokens.apply(do_lematization, args=(my_lemmatizer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>post</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stem</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>316528</th>\n",
       "      <td>INFP</td>\n",
       "      <td>yup my istj buddy takes thorough notes and doe...</td>\n",
       "      <td>[yup, my, istj, buddy, takes, thorough, notes,...</td>\n",
       "      <td>[yup, my, istj, buddi, take, thorough, note, a...</td>\n",
       "      <td>[yup, my, istj, buddy, take, thorough, note, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316529</th>\n",
       "      <td>INFP</td>\n",
       "      <td>yup my istj buddy takes thorough notes and doe...</td>\n",
       "      <td>[yup, my, istj, buddy, takes, thorough, notes,...</td>\n",
       "      <td>[yup, my, istj, buddi, take, thorough, note, a...</td>\n",
       "      <td>[yup, my, istj, buddy, take, thorough, note, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316530</th>\n",
       "      <td>INFP</td>\n",
       "      <td>i seem to have touched a nerve ooo proud being...</td>\n",
       "      <td>[i, seem, to, have, touched, a, nerve, ooo, pr...</td>\n",
       "      <td>[i, seem, to, have, touch, a, nerv, ooo, proud...</td>\n",
       "      <td>[i, seem, to, have, touched, a, nerve, ooo, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316531</th>\n",
       "      <td>INFP</td>\n",
       "      <td>on asking an infp for advice go not to the elv...</td>\n",
       "      <td>[on, asking, an, infp, for, advice, go, not, t...</td>\n",
       "      <td>[on, ask, an, infp, for, advic, go, not, to, t...</td>\n",
       "      <td>[on, asking, an, infp, for, advice, go, not, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316532</th>\n",
       "      <td>INFP</td>\n",
       "      <td>all the xxxjs they make plans and get stressed...</td>\n",
       "      <td>[all, the, xxxjs, they, make, plans, and, get,...</td>\n",
       "      <td>[all, the, xxxjs, they, make, plan, and, get, ...</td>\n",
       "      <td>[all, the, xxxjs, they, make, plan, and, get, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316533</th>\n",
       "      <td>INFP</td>\n",
       "      <td>istp guys guys chill we arent trying to guess ...</td>\n",
       "      <td>[istp, guys, guys, chill, we, arent, trying, t...</td>\n",
       "      <td>[istp, guy, guy, chill, we, arent, tri, to, gu...</td>\n",
       "      <td>[istp, guy, guy, chill, we, arent, trying, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316534</th>\n",
       "      <td>INFP</td>\n",
       "      <td>all cats are intjs dogs are probably esfps i c...</td>\n",
       "      <td>[all, cats, are, intjs, dogs, are, probably, e...</td>\n",
       "      <td>[all, cat, are, intj, dog, are, probabl, esfp,...</td>\n",
       "      <td>[all, cat, are, intjs, dog, are, probably, esf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316535</th>\n",
       "      <td>INFP</td>\n",
       "      <td>hello all ive been absent for a while firstly ...</td>\n",
       "      <td>[hello, all, ive, been, absent, for, a, while,...</td>\n",
       "      <td>[hello, all, ive, been, absent, for, a, while,...</td>\n",
       "      <td>[hello, all, ive, been, absent, for, a, while,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316536</th>\n",
       "      <td>INFP</td>\n",
       "      <td>1 type  infp  2 text style  i tend to either u...</td>\n",
       "      <td>[1, type, infp, 2, text, style, i, tend, to, e...</td>\n",
       "      <td>[1, type, infp, 2, text, style, i, tend, to, e...</td>\n",
       "      <td>[1, type, infp, 2, text, style, i, tend, to, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316537</th>\n",
       "      <td>INFP</td>\n",
       "      <td>yes she does know him but even less than i do ...</td>\n",
       "      <td>[yes, she, does, know, him, but, even, less, t...</td>\n",
       "      <td>[yes, she, doe, know, him, but, even, less, th...</td>\n",
       "      <td>[yes, she, doe, know, him, but, even, less, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316538</th>\n",
       "      <td>INFP</td>\n",
       "      <td>hello istjs i the friendly infp have a questio...</td>\n",
       "      <td>[hello, istjs, i, the, friendly, infp, have, a...</td>\n",
       "      <td>[hello, istj, i, the, friend, infp, have, a, q...</td>\n",
       "      <td>[hello, istjs, i, the, friendly, infp, have, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316539</th>\n",
       "      <td>INFP</td>\n",
       "      <td>my family my dad is an infp he was adopted and...</td>\n",
       "      <td>[my, family, my, dad, is, an, infp, he, was, a...</td>\n",
       "      <td>[my, famili, my, dad, is, an, infp, he, was, a...</td>\n",
       "      <td>[my, family, my, dad, is, an, infp, he, wa, ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316540</th>\n",
       "      <td>INFP</td>\n",
       "      <td>two melons are talking and the first melon say...</td>\n",
       "      <td>[two, melons, are, talking, and, the, first, m...</td>\n",
       "      <td>[two, melon, are, talk, and, the, first, melon...</td>\n",
       "      <td>[two, melon, are, talking, and, the, first, me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316541</th>\n",
       "      <td>INFP</td>\n",
       "      <td>an infp turns on the imperial march from star ...</td>\n",
       "      <td>[an, infp, turns, on, the, imperial, march, fr...</td>\n",
       "      <td>[an, infp, turn, on, the, imperi, march, from,...</td>\n",
       "      <td>[an, infp, turn, on, the, imperial, march, fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316542</th>\n",
       "      <td>INFP</td>\n",
       "      <td>also infps  and infjs react to problems differ...</td>\n",
       "      <td>[also, infps, and, infjs, react, to, problems,...</td>\n",
       "      <td>[also, infp, and, infj, react, to, problem, di...</td>\n",
       "      <td>[also, infps, and, infjs, react, to, problem, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316543</th>\n",
       "      <td>INFP</td>\n",
       "      <td>kallinhausin you may have just rooted out the ...</td>\n",
       "      <td>[kallinhausin, you, may, have, just, rooted, o...</td>\n",
       "      <td>[kallinhausin, you, may, have, just, root, out...</td>\n",
       "      <td>[kallinhausin, you, may, have, just, rooted, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316544</th>\n",
       "      <td>INFP</td>\n",
       "      <td>in regards to the king in the show not in the ...</td>\n",
       "      <td>[in, regards, to, the, king, in, the, show, no...</td>\n",
       "      <td>[in, regard, to, the, king, in, the, show, not...</td>\n",
       "      <td>[in, regard, to, the, king, in, the, show, not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316545</th>\n",
       "      <td>INFP</td>\n",
       "      <td>sunlight bouncing off the fog at dawn  serendi...</td>\n",
       "      <td>[sunlight, bouncing, off, the, fog, at, dawn, ...</td>\n",
       "      <td>[sunlight, bounc, off, the, fog, at, dawn, ser...</td>\n",
       "      <td>[sunlight, bouncing, off, the, fog, at, dawn, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316546</th>\n",
       "      <td>INFP</td>\n",
       "      <td>songs are really powerful</td>\n",
       "      <td>[songs, are, really, powerful]</td>\n",
       "      <td>[song, are, realli, power]</td>\n",
       "      <td>[song, are, really, powerful]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316547</th>\n",
       "      <td>INFP</td>\n",
       "      <td>i just have to remember they werent trying to ...</td>\n",
       "      <td>[i, just, have, to, remember, they, werent, tr...</td>\n",
       "      <td>[i, just, have, to, rememb, they, werent, tri,...</td>\n",
       "      <td>[i, just, have, to, remember, they, werent, tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        type                                               post  \\\n",
       "316528  INFP  yup my istj buddy takes thorough notes and doe...   \n",
       "316529  INFP  yup my istj buddy takes thorough notes and doe...   \n",
       "316530  INFP  i seem to have touched a nerve ooo proud being...   \n",
       "316531  INFP  on asking an infp for advice go not to the elv...   \n",
       "316532  INFP  all the xxxjs they make plans and get stressed...   \n",
       "316533  INFP  istp guys guys chill we arent trying to guess ...   \n",
       "316534  INFP  all cats are intjs dogs are probably esfps i c...   \n",
       "316535  INFP  hello all ive been absent for a while firstly ...   \n",
       "316536  INFP  1 type  infp  2 text style  i tend to either u...   \n",
       "316537  INFP  yes she does know him but even less than i do ...   \n",
       "316538  INFP  hello istjs i the friendly infp have a questio...   \n",
       "316539  INFP  my family my dad is an infp he was adopted and...   \n",
       "316540  INFP  two melons are talking and the first melon say...   \n",
       "316541  INFP  an infp turns on the imperial march from star ...   \n",
       "316542  INFP  also infps  and infjs react to problems differ...   \n",
       "316543  INFP  kallinhausin you may have just rooted out the ...   \n",
       "316544  INFP  in regards to the king in the show not in the ...   \n",
       "316545  INFP  sunlight bouncing off the fog at dawn  serendi...   \n",
       "316546  INFP                          songs are really powerful   \n",
       "316547  INFP  i just have to remember they werent trying to ...   \n",
       "\n",
       "                                                   tokens  \\\n",
       "316528  [yup, my, istj, buddy, takes, thorough, notes,...   \n",
       "316529  [yup, my, istj, buddy, takes, thorough, notes,...   \n",
       "316530  [i, seem, to, have, touched, a, nerve, ooo, pr...   \n",
       "316531  [on, asking, an, infp, for, advice, go, not, t...   \n",
       "316532  [all, the, xxxjs, they, make, plans, and, get,...   \n",
       "316533  [istp, guys, guys, chill, we, arent, trying, t...   \n",
       "316534  [all, cats, are, intjs, dogs, are, probably, e...   \n",
       "316535  [hello, all, ive, been, absent, for, a, while,...   \n",
       "316536  [1, type, infp, 2, text, style, i, tend, to, e...   \n",
       "316537  [yes, she, does, know, him, but, even, less, t...   \n",
       "316538  [hello, istjs, i, the, friendly, infp, have, a...   \n",
       "316539  [my, family, my, dad, is, an, infp, he, was, a...   \n",
       "316540  [two, melons, are, talking, and, the, first, m...   \n",
       "316541  [an, infp, turns, on, the, imperial, march, fr...   \n",
       "316542  [also, infps, and, infjs, react, to, problems,...   \n",
       "316543  [kallinhausin, you, may, have, just, rooted, o...   \n",
       "316544  [in, regards, to, the, king, in, the, show, no...   \n",
       "316545  [sunlight, bouncing, off, the, fog, at, dawn, ...   \n",
       "316546                     [songs, are, really, powerful]   \n",
       "316547  [i, just, have, to, remember, they, werent, tr...   \n",
       "\n",
       "                                                     stem  \\\n",
       "316528  [yup, my, istj, buddi, take, thorough, note, a...   \n",
       "316529  [yup, my, istj, buddi, take, thorough, note, a...   \n",
       "316530  [i, seem, to, have, touch, a, nerv, ooo, proud...   \n",
       "316531  [on, ask, an, infp, for, advic, go, not, to, t...   \n",
       "316532  [all, the, xxxjs, they, make, plan, and, get, ...   \n",
       "316533  [istp, guy, guy, chill, we, arent, tri, to, gu...   \n",
       "316534  [all, cat, are, intj, dog, are, probabl, esfp,...   \n",
       "316535  [hello, all, ive, been, absent, for, a, while,...   \n",
       "316536  [1, type, infp, 2, text, style, i, tend, to, e...   \n",
       "316537  [yes, she, doe, know, him, but, even, less, th...   \n",
       "316538  [hello, istj, i, the, friend, infp, have, a, q...   \n",
       "316539  [my, famili, my, dad, is, an, infp, he, was, a...   \n",
       "316540  [two, melon, are, talk, and, the, first, melon...   \n",
       "316541  [an, infp, turn, on, the, imperi, march, from,...   \n",
       "316542  [also, infp, and, infj, react, to, problem, di...   \n",
       "316543  [kallinhausin, you, may, have, just, root, out...   \n",
       "316544  [in, regard, to, the, king, in, the, show, not...   \n",
       "316545  [sunlight, bounc, off, the, fog, at, dawn, ser...   \n",
       "316546                         [song, are, realli, power]   \n",
       "316547  [i, just, have, to, rememb, they, werent, tri,...   \n",
       "\n",
       "                                                    lemma  \n",
       "316528  [yup, my, istj, buddy, take, thorough, note, a...  \n",
       "316529  [yup, my, istj, buddy, take, thorough, note, a...  \n",
       "316530  [i, seem, to, have, touched, a, nerve, ooo, pr...  \n",
       "316531  [on, asking, an, infp, for, advice, go, not, t...  \n",
       "316532  [all, the, xxxjs, they, make, plan, and, get, ...  \n",
       "316533  [istp, guy, guy, chill, we, arent, trying, to,...  \n",
       "316534  [all, cat, are, intjs, dog, are, probably, esf...  \n",
       "316535  [hello, all, ive, been, absent, for, a, while,...  \n",
       "316536  [1, type, infp, 2, text, style, i, tend, to, e...  \n",
       "316537  [yes, she, doe, know, him, but, even, less, th...  \n",
       "316538  [hello, istjs, i, the, friendly, infp, have, a...  \n",
       "316539  [my, family, my, dad, is, an, infp, he, wa, ad...  \n",
       "316540  [two, melon, are, talking, and, the, first, me...  \n",
       "316541  [an, infp, turn, on, the, imperial, march, fro...  \n",
       "316542  [also, infps, and, infjs, react, to, problem, ...  \n",
       "316543  [kallinhausin, you, may, have, just, rooted, o...  \n",
       "316544  [in, regard, to, the, king, in, the, show, not...  \n",
       "316545  [sunlight, bouncing, off, the, fog, at, dawn, ...  \n",
       "316546                      [song, are, really, powerful]  \n",
       "316547  [i, just, have, to, remember, they, werent, tr...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer_ = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dancing'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer_.lemmatize(\"dancing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A8ejyfxbVAAK",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def mbti_lemma(words, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sxUiwXfIVAAN",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_mbti['lemma'] = all_mbti['tokens'].apply(mbti_lemma, args=(lemmatizer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>post</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stem</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>316528</th>\n",
       "      <td>INFP</td>\n",
       "      <td>yup my istj buddy takes thorough notes and doe...</td>\n",
       "      <td>[yup, my, istj, buddy, takes, thorough, notes,...</td>\n",
       "      <td>[yup, my, istj, buddi, take, thorough, note, a...</td>\n",
       "      <td>[yup, my, istj, buddy, take, thorough, note, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316529</th>\n",
       "      <td>INFP</td>\n",
       "      <td>yup my istj buddy takes thorough notes and doe...</td>\n",
       "      <td>[yup, my, istj, buddy, takes, thorough, notes,...</td>\n",
       "      <td>[yup, my, istj, buddi, take, thorough, note, a...</td>\n",
       "      <td>[yup, my, istj, buddy, take, thorough, note, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316530</th>\n",
       "      <td>INFP</td>\n",
       "      <td>i seem to have touched a nerve ooo proud being...</td>\n",
       "      <td>[i, seem, to, have, touched, a, nerve, ooo, pr...</td>\n",
       "      <td>[i, seem, to, have, touch, a, nerv, ooo, proud...</td>\n",
       "      <td>[i, seem, to, have, touched, a, nerve, ooo, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316531</th>\n",
       "      <td>INFP</td>\n",
       "      <td>on asking an infp for advice go not to the elv...</td>\n",
       "      <td>[on, asking, an, infp, for, advice, go, not, t...</td>\n",
       "      <td>[on, ask, an, infp, for, advic, go, not, to, t...</td>\n",
       "      <td>[on, asking, an, infp, for, advice, go, not, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316532</th>\n",
       "      <td>INFP</td>\n",
       "      <td>all the xxxjs they make plans and get stressed...</td>\n",
       "      <td>[all, the, xxxjs, they, make, plans, and, get,...</td>\n",
       "      <td>[all, the, xxxjs, they, make, plan, and, get, ...</td>\n",
       "      <td>[all, the, xxxjs, they, make, plan, and, get, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316533</th>\n",
       "      <td>INFP</td>\n",
       "      <td>istp guys guys chill we arent trying to guess ...</td>\n",
       "      <td>[istp, guys, guys, chill, we, arent, trying, t...</td>\n",
       "      <td>[istp, guy, guy, chill, we, arent, tri, to, gu...</td>\n",
       "      <td>[istp, guy, guy, chill, we, arent, trying, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316534</th>\n",
       "      <td>INFP</td>\n",
       "      <td>all cats are intjs dogs are probably esfps i c...</td>\n",
       "      <td>[all, cats, are, intjs, dogs, are, probably, e...</td>\n",
       "      <td>[all, cat, are, intj, dog, are, probabl, esfp,...</td>\n",
       "      <td>[all, cat, are, intjs, dog, are, probably, esf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316535</th>\n",
       "      <td>INFP</td>\n",
       "      <td>hello all ive been absent for a while firstly ...</td>\n",
       "      <td>[hello, all, ive, been, absent, for, a, while,...</td>\n",
       "      <td>[hello, all, ive, been, absent, for, a, while,...</td>\n",
       "      <td>[hello, all, ive, been, absent, for, a, while,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316536</th>\n",
       "      <td>INFP</td>\n",
       "      <td>1 type  infp  2 text style  i tend to either u...</td>\n",
       "      <td>[1, type, infp, 2, text, style, i, tend, to, e...</td>\n",
       "      <td>[1, type, infp, 2, text, style, i, tend, to, e...</td>\n",
       "      <td>[1, type, infp, 2, text, style, i, tend, to, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316537</th>\n",
       "      <td>INFP</td>\n",
       "      <td>yes she does know him but even less than i do ...</td>\n",
       "      <td>[yes, she, does, know, him, but, even, less, t...</td>\n",
       "      <td>[yes, she, doe, know, him, but, even, less, th...</td>\n",
       "      <td>[yes, she, doe, know, him, but, even, less, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316538</th>\n",
       "      <td>INFP</td>\n",
       "      <td>hello istjs i the friendly infp have a questio...</td>\n",
       "      <td>[hello, istjs, i, the, friendly, infp, have, a...</td>\n",
       "      <td>[hello, istj, i, the, friend, infp, have, a, q...</td>\n",
       "      <td>[hello, istjs, i, the, friendly, infp, have, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316539</th>\n",
       "      <td>INFP</td>\n",
       "      <td>my family my dad is an infp he was adopted and...</td>\n",
       "      <td>[my, family, my, dad, is, an, infp, he, was, a...</td>\n",
       "      <td>[my, famili, my, dad, is, an, infp, he, was, a...</td>\n",
       "      <td>[my, family, my, dad, is, an, infp, he, wa, ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316540</th>\n",
       "      <td>INFP</td>\n",
       "      <td>two melons are talking and the first melon say...</td>\n",
       "      <td>[two, melons, are, talking, and, the, first, m...</td>\n",
       "      <td>[two, melon, are, talk, and, the, first, melon...</td>\n",
       "      <td>[two, melon, are, talking, and, the, first, me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316541</th>\n",
       "      <td>INFP</td>\n",
       "      <td>an infp turns on the imperial march from star ...</td>\n",
       "      <td>[an, infp, turns, on, the, imperial, march, fr...</td>\n",
       "      <td>[an, infp, turn, on, the, imperi, march, from,...</td>\n",
       "      <td>[an, infp, turn, on, the, imperial, march, fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316542</th>\n",
       "      <td>INFP</td>\n",
       "      <td>also infps  and infjs react to problems differ...</td>\n",
       "      <td>[also, infps, and, infjs, react, to, problems,...</td>\n",
       "      <td>[also, infp, and, infj, react, to, problem, di...</td>\n",
       "      <td>[also, infps, and, infjs, react, to, problem, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316543</th>\n",
       "      <td>INFP</td>\n",
       "      <td>kallinhausin you may have just rooted out the ...</td>\n",
       "      <td>[kallinhausin, you, may, have, just, rooted, o...</td>\n",
       "      <td>[kallinhausin, you, may, have, just, root, out...</td>\n",
       "      <td>[kallinhausin, you, may, have, just, rooted, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316544</th>\n",
       "      <td>INFP</td>\n",
       "      <td>in regards to the king in the show not in the ...</td>\n",
       "      <td>[in, regards, to, the, king, in, the, show, no...</td>\n",
       "      <td>[in, regard, to, the, king, in, the, show, not...</td>\n",
       "      <td>[in, regard, to, the, king, in, the, show, not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316545</th>\n",
       "      <td>INFP</td>\n",
       "      <td>sunlight bouncing off the fog at dawn  serendi...</td>\n",
       "      <td>[sunlight, bouncing, off, the, fog, at, dawn, ...</td>\n",
       "      <td>[sunlight, bounc, off, the, fog, at, dawn, ser...</td>\n",
       "      <td>[sunlight, bouncing, off, the, fog, at, dawn, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316546</th>\n",
       "      <td>INFP</td>\n",
       "      <td>songs are really powerful</td>\n",
       "      <td>[songs, are, really, powerful]</td>\n",
       "      <td>[song, are, realli, power]</td>\n",
       "      <td>[song, are, really, powerful]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316547</th>\n",
       "      <td>INFP</td>\n",
       "      <td>i just have to remember they werent trying to ...</td>\n",
       "      <td>[i, just, have, to, remember, they, werent, tr...</td>\n",
       "      <td>[i, just, have, to, rememb, they, werent, tri,...</td>\n",
       "      <td>[i, just, have, to, remember, they, werent, tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        type                                               post  \\\n",
       "316528  INFP  yup my istj buddy takes thorough notes and doe...   \n",
       "316529  INFP  yup my istj buddy takes thorough notes and doe...   \n",
       "316530  INFP  i seem to have touched a nerve ooo proud being...   \n",
       "316531  INFP  on asking an infp for advice go not to the elv...   \n",
       "316532  INFP  all the xxxjs they make plans and get stressed...   \n",
       "316533  INFP  istp guys guys chill we arent trying to guess ...   \n",
       "316534  INFP  all cats are intjs dogs are probably esfps i c...   \n",
       "316535  INFP  hello all ive been absent for a while firstly ...   \n",
       "316536  INFP  1 type  infp  2 text style  i tend to either u...   \n",
       "316537  INFP  yes she does know him but even less than i do ...   \n",
       "316538  INFP  hello istjs i the friendly infp have a questio...   \n",
       "316539  INFP  my family my dad is an infp he was adopted and...   \n",
       "316540  INFP  two melons are talking and the first melon say...   \n",
       "316541  INFP  an infp turns on the imperial march from star ...   \n",
       "316542  INFP  also infps  and infjs react to problems differ...   \n",
       "316543  INFP  kallinhausin you may have just rooted out the ...   \n",
       "316544  INFP  in regards to the king in the show not in the ...   \n",
       "316545  INFP  sunlight bouncing off the fog at dawn  serendi...   \n",
       "316546  INFP                          songs are really powerful   \n",
       "316547  INFP  i just have to remember they werent trying to ...   \n",
       "\n",
       "                                                   tokens  \\\n",
       "316528  [yup, my, istj, buddy, takes, thorough, notes,...   \n",
       "316529  [yup, my, istj, buddy, takes, thorough, notes,...   \n",
       "316530  [i, seem, to, have, touched, a, nerve, ooo, pr...   \n",
       "316531  [on, asking, an, infp, for, advice, go, not, t...   \n",
       "316532  [all, the, xxxjs, they, make, plans, and, get,...   \n",
       "316533  [istp, guys, guys, chill, we, arent, trying, t...   \n",
       "316534  [all, cats, are, intjs, dogs, are, probably, e...   \n",
       "316535  [hello, all, ive, been, absent, for, a, while,...   \n",
       "316536  [1, type, infp, 2, text, style, i, tend, to, e...   \n",
       "316537  [yes, she, does, know, him, but, even, less, t...   \n",
       "316538  [hello, istjs, i, the, friendly, infp, have, a...   \n",
       "316539  [my, family, my, dad, is, an, infp, he, was, a...   \n",
       "316540  [two, melons, are, talking, and, the, first, m...   \n",
       "316541  [an, infp, turns, on, the, imperial, march, fr...   \n",
       "316542  [also, infps, and, infjs, react, to, problems,...   \n",
       "316543  [kallinhausin, you, may, have, just, rooted, o...   \n",
       "316544  [in, regards, to, the, king, in, the, show, no...   \n",
       "316545  [sunlight, bouncing, off, the, fog, at, dawn, ...   \n",
       "316546                     [songs, are, really, powerful]   \n",
       "316547  [i, just, have, to, remember, they, werent, tr...   \n",
       "\n",
       "                                                     stem  \\\n",
       "316528  [yup, my, istj, buddi, take, thorough, note, a...   \n",
       "316529  [yup, my, istj, buddi, take, thorough, note, a...   \n",
       "316530  [i, seem, to, have, touch, a, nerv, ooo, proud...   \n",
       "316531  [on, ask, an, infp, for, advic, go, not, to, t...   \n",
       "316532  [all, the, xxxjs, they, make, plan, and, get, ...   \n",
       "316533  [istp, guy, guy, chill, we, arent, tri, to, gu...   \n",
       "316534  [all, cat, are, intj, dog, are, probabl, esfp,...   \n",
       "316535  [hello, all, ive, been, absent, for, a, while,...   \n",
       "316536  [1, type, infp, 2, text, style, i, tend, to, e...   \n",
       "316537  [yes, she, doe, know, him, but, even, less, th...   \n",
       "316538  [hello, istj, i, the, friend, infp, have, a, q...   \n",
       "316539  [my, famili, my, dad, is, an, infp, he, was, a...   \n",
       "316540  [two, melon, are, talk, and, the, first, melon...   \n",
       "316541  [an, infp, turn, on, the, imperi, march, from,...   \n",
       "316542  [also, infp, and, infj, react, to, problem, di...   \n",
       "316543  [kallinhausin, you, may, have, just, root, out...   \n",
       "316544  [in, regard, to, the, king, in, the, show, not...   \n",
       "316545  [sunlight, bounc, off, the, fog, at, dawn, ser...   \n",
       "316546                         [song, are, realli, power]   \n",
       "316547  [i, just, have, to, rememb, they, werent, tri,...   \n",
       "\n",
       "                                                    lemma  \n",
       "316528  [yup, my, istj, buddy, take, thorough, note, a...  \n",
       "316529  [yup, my, istj, buddy, take, thorough, note, a...  \n",
       "316530  [i, seem, to, have, touched, a, nerve, ooo, pr...  \n",
       "316531  [on, asking, an, infp, for, advice, go, not, t...  \n",
       "316532  [all, the, xxxjs, they, make, plan, and, get, ...  \n",
       "316533  [istp, guy, guy, chill, we, arent, trying, to,...  \n",
       "316534  [all, cat, are, intjs, dog, are, probably, esf...  \n",
       "316535  [hello, all, ive, been, absent, for, a, while,...  \n",
       "316536  [1, type, infp, 2, text, style, i, tend, to, e...  \n",
       "316537  [yes, she, doe, know, him, but, even, less, th...  \n",
       "316538  [hello, istjs, i, the, friendly, infp, have, a...  \n",
       "316539  [my, family, my, dad, is, an, infp, he, wa, ad...  \n",
       "316540  [two, melon, are, talking, and, the, first, me...  \n",
       "316541  [an, infp, turn, on, the, imperial, march, fro...  \n",
       "316542  [also, infps, and, infjs, react, to, problem, ...  \n",
       "316543  [kallinhausin, you, may, have, just, rooted, o...  \n",
       "316544  [in, regard, to, the, king, in, the, show, not...  \n",
       "316545  [sunlight, bouncing, off, the, fog, at, dawn, ...  \n",
       "316546                      [song, are, really, powerful]  \n",
       "316547  [i, just, have, to, remember, they, werent, tr...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mbti.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will print out the results of the lemmatization to see what we have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3loMYtFVAAO",
    "outputId": "12581d00-60bf-409e-b554-02530fd535b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i                    --> i         \n",
      "hate                 --> hate      \n",
      "april                --> april     \n",
      "fools                --> fool      \n",
      "day                  --> day       \n",
      "angry                --> angry     \n",
      "theres               --> there     \n",
      "a                    --> a         \n",
      "site                 --> site      \n",
      "im                   --> im        \n",
      "regularly            --> regularly \n",
      "on                   --> on        \n",
      "and                  --> and       \n",
      "the                  --> the       \n",
      "admins               --> admins    \n",
      "are                  --> are       \n",
      "screwing             --> screwing  \n",
      "everything           --> everything\n",
      "up                   --> up        \n",
      "today                --> today     \n",
      "for                  --> for       \n",
      "a                    --> a         \n",
      "laugh                --> laugh     \n",
      "but                  --> but       \n",
      "i                    --> i         \n",
      "dont                 --> dont      \n",
      "find                 --> find      \n",
      "it                   --> it        \n",
      "funny                --> funny     \n",
      "im                   --> im        \n",
      "actually             --> actually  \n",
      "quite                --> quite     \n",
      "freaked              --> freaked   \n",
      "out                  --> out       \n",
      "about                --> about     \n",
      "it                   --> it        \n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(all_mbti.iloc[268702]['tokens']):    \n",
    "    print ('{:20s} --> {:10s}'.format(t, all_mbti.iloc[268702]['lemma'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UZomXVzoVAAR"
   },
   "source": [
    "### Stop words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_7g3o3SVAAR"
   },
   "source": [
    "Stop words are words that do not contain important significance to be used in search queries. Usually, these words are filtered out from search queries because they return a vast amount of unnecessary information. `nltk` has a corpus of stop words. Let's print out the stop words for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XY1p3uZVAAS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rD6ovPGLVAAW",
    "outputId": "aae2892f-0e10-4325-f3a7-3e7091352282"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(stopwords.words('english'))[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stops words because they add little-to-no semantic to the text \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "all_stopwords = stopwords.words(\"english\")\n",
    "all_stopwords.__len__() \n",
    "\n",
    "# Implementing a function to remove all the stop words \n",
    "def do_stopWordRemoval(word_list):\n",
    "    print(\"working...\")\n",
    "    return [word for word in word_list if word not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n"
     ]
    }
   ],
   "source": [
    "dataset[\"no_stopwords\"] = dataset.lemma.apply(do_stopWordRemoval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function removes all of the English stop words from the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPDdMJ2SVAAX",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):    \n",
    "    return [t for t in tokens if t not in stopwords.words('english')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5PBwZRUVAAb"
   },
   "source": [
    "To remove stop words, simply uncomment and execute the following cell! Please note, however, that this process may take a while due to the computational load imposed by the Pandas apply function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kyeHeXlAVAAb",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# all_mbti['stem'] = all_mbti['tokens'].apply(remove_stop_words)\n",
    "#all_mbti['stem']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "In conclusion, mastering text-cleaning techniques and feature extraction in natural language processing (NLP) is crucial for effectively preparing unstructured text data for analysis and machine learning tasks. Through methods such as removing noise like URLs, converting text to lowercase, removing punctuation, tokenisation, stemming, lemmatization, and stop word removal, we can enhance the quality of text data and extract meaningful features for further analysis. These techniques play a fundamental role in NLP pipelines, enabling the development of robust models and gaining valuable insights from textual data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VCXae5QXU__Z",
    "wzM8TbWBU__h",
    "FvA-QZmRU__r",
    "hAUkklVXU__6",
    "rFln-NFtVAAI",
    "UZomXVzoVAAR",
    "qp-n688CVAAc",
    "tGmGzrbsVAAf",
    "oFzCFS89VABM",
    "TlO1q-zlVABg"
   ],
   "name": "3_How-do-machines-understand language.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
